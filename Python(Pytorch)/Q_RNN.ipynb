{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_RNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers):\n",
    "        super(Q_RNN, self).__init__()\n",
    "        self.r1=nn.RNN(input_size, hidden_size,num_layers, nonlinearity='tanh')\n",
    "        self.fc1 = nn.Linear(hidden_size,8)\n",
    "        self.fc2 = nn.Linear(8,3)\n",
    "    def forward(self,X0,num_layers,hidden_size): \n",
    "        h0=torch.zeros([num_layers,X0.shape[1],hidden_size])  \n",
    "        output, hn = self.r1(X0,h0)\n",
    "        Q=self.fc1(output)\n",
    "        Q=F.leaky_relu(Q,0.1, True)\n",
    "        Q=self.fc2(Q)\n",
    "        q=Q[-1,:,:]\n",
    "        return Q,hn,q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(lr=2e-2,input_size=2, hidden_size=4,num_layers=1):\n",
    "    model = Q_RNN(input_size,hidden_size,num_layers)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(S,e=1e-2,num_layers=1,hidden_size=4):\n",
    "    s=torch.tensor(S,dtype=torch.float32)\n",
    "    S=Variable(s,requires_grad=True)\n",
    "    _,_,V=model.forward(S,num_layers,hidden_size)\n",
    "    V=V[-1,:]\n",
    "    v,ind = V.max(0)\n",
    "    aa=[i for i, j in enumerate(V) if j == v]\n",
    "    a=0\n",
    "    if np.random.rand(1)<=1-e:\n",
    "        if len(aa)==1:\n",
    "            a=ind.item()+1\n",
    "        elif 0 in aa:\n",
    "            a=1\n",
    "        else:\n",
    "            a=ind.item()+1\n",
    "    else:\n",
    "        a=np.random.randint(1,4)\n",
    "    a=torch.tensor(a,dtype=torch.float32)\n",
    "    if np.isnan(a):\n",
    "        print('NNNNNNNAn')\n",
    "    return a,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Environment(f=0.1,R_plus=1,R_minus=20,e=1e-2,max_toss=8000,hidden_size=8,num_layers=2):\n",
    "    model.eval()\n",
    "    S_0=np.zeros([1,1,2])\n",
    "    a_0=np.zeros([1,1,2])\n",
    "    replay=[]\n",
    "    Rplay=[]\n",
    "    coin=np.random.randint(2,4)\n",
    "\n",
    "    if coin==3:\n",
    "        S_list=S_0\n",
    "        o_list=S_0\n",
    "        i=0\n",
    "        while i<max_toss:\n",
    "            a,_=e_greedy(o_list,e=e,num_layers=num_layers,hidden_size=hidden_size)\n",
    "            #print(a)\n",
    "            i=i+1\n",
    "            if a!=1:\n",
    "                if a==coin:\n",
    "                    Reward=R_plus\n",
    "                else:\n",
    "                    Reward=-R_minus\n",
    "                Rplay.append([i-1,a,Reward,i-1])\n",
    "                if S_list.ndim==1:\n",
    "                    replay.append([S_list,a,Reward,S_list])\n",
    "                else:\n",
    "                    replay.append([S_list[-1,:],a,Reward,S_list[-1,:]])\n",
    "                break\n",
    "            else:\n",
    "                #rint(S_list)\n",
    "                if np.random.rand(1)<1-f:\n",
    "                    S_list=np.concatenate((S_list,np.array([[[0,1]]])),axis=0)\n",
    "                    o_list=np.concatenate((o_list,np.array([[[0,1]]])),axis=0)\n",
    "                    S_list[-1,:,:]=S_list[-1,:,:]+S_list[-2,:,:]\n",
    "                else:\n",
    "                    S_list=np.concatenate((S_list,np.array([[[1,0]]])),axis=0)\n",
    "                    o_list=np.concatenate((o_list,np.array([[[1,0]]])),axis=0)\n",
    "                    S_list[-1,:,:]=S_list[-1,:,:]+S_list[-2,:,:] \n",
    "                Reward=0   \n",
    "                Rplay.append([i-1,a,Reward,i])\n",
    "                replay.append([S_list[-2,:],a,Reward,S_list[-1,:]])\n",
    "    else:\n",
    "        S_list=S_0\n",
    "        o_list=S_0\n",
    "        i=0\n",
    "        while i<max_toss:\n",
    "            a,_=e_greedy(o_list,e=e,num_layers=num_layers,hidden_size=hidden_size)\n",
    "            #print(a)\n",
    "            i=i+1\n",
    "            if a!=1:\n",
    "                if a==coin:\n",
    "                    Reward=R_plus\n",
    "                else:\n",
    "                    Reward=-R_minus\n",
    "                Rplay.append([i-1,a,Reward,i-1])\n",
    "                if S_list.ndim==1:\n",
    "                    replay.append([S_list,a,Reward,S_list])\n",
    "                else:\n",
    "                    replay.append([S_list[-1,:],a,Reward,S_list[-1,:]])\n",
    "                break\n",
    "            else:\n",
    "                #print(S_list)\n",
    "                if np.random.rand(1)<1-f:\n",
    "                    S_list=np.concatenate((S_list,np.array([[[1,0]]])),axis=0)\n",
    "                    o_list=np.concatenate((o_list,np.array([[[1,0]]])),axis=0)\n",
    "                    S_list[-1,:,:]=S_list[-1,:,:]+S_list[-2,:,:]\n",
    "                else:\n",
    "                    S_list=np.concatenate((S_list,np.array([[[0,1]]])),axis=0)\n",
    "                    o_list=np.concatenate((o_list,np.array([[[0,1]]])),axis=0)\n",
    "                    S_list[-1,:,:]=S_list[-1,:,:]+S_list[-2,:,:]\n",
    "                Reward=0\n",
    "                Rplay.append([i-1,a,Reward,i])\n",
    "                replay.append([S_list[-2,:],a,Reward,S_list[-1,:]])\n",
    "    R=Reward/max((len(S_list)-1),1)\n",
    "    for x in Rplay[:-1]:\n",
    "        x[2]=-Reward/(len(S_list)-1)**2*(len(S_list)-2)\n",
    "    return replay,S_list,Rplay,o_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(Rplay,Q):\n",
    "    Target=Q\n",
    "    for i,R in enumerate(Rplay):\n",
    "        Target[i,:,np.int32(R[1].cpu().numpy()-1)]=R[2]\n",
    "    return Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, Rplay,A_list,optimizer,hidden_size=4,num_layers=1):\n",
    "    gamma=0.9\n",
    "    losses=list()\n",
    "    A_list=torch.tensor(A_list,dtype=torch.float32)\n",
    "    A_list=Variable(A_list,requires_grad=True)\n",
    "    Q,_,_=model(A_list,num_layers,hidden_size) \n",
    "    Target=get_target(Rplay,Q)   \n",
    "\n",
    "    model.train()\n",
    "    for j in range(1):\n",
    "        # train        \n",
    "        optimizer.zero_grad()\n",
    "        Q,_,_=model(A_list,num_layers,hidden_size)  \n",
    "        loss = F.mse_loss(Q, Target)\n",
    "        #print(loss)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, optimizer=setup(hidden_size=8,num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66c8b12af504914ae114e7187ed67d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss in epoch 0: 136.90639\n",
      "Average loss in epoch 100: 1.41397\n",
      "Average loss in epoch 200: 0.19934\n",
      "Average loss in epoch 300: 0.10408\n",
      "Average loss in epoch 400: 0.50911\n",
      "Average loss in epoch 500: 0.95753\n",
      "Average loss in epoch 600: 0.22325\n",
      "Average loss in epoch 700: 0.25765\n",
      "Average loss in epoch 800: 1.17792\n",
      "Average loss in epoch 900: 3.91364\n",
      "Average loss in epoch 1000: 1.01740\n",
      "Average loss in epoch 1100: 0.10024\n",
      "Average loss in epoch 1200: 0.22798\n",
      "Average loss in epoch 1300: 0.48575\n",
      "Average loss in epoch 1400: 0.11087\n",
      "Average loss in epoch 1500: 0.15894\n",
      "Average loss in epoch 1600: 0.05780\n",
      "Average loss in epoch 1700: 0.03770\n"
     ]
    }
   ],
   "source": [
    "N=10000\n",
    "for epoch in tqdm(range(N)):\n",
    "    _,_,Rp,al=Environment(hidden_size=8,num_layers=2)\n",
    "    epoch_losses =train_epoch(model, Rp,al,optimizer,hidden_size=8,num_layers=2)\n",
    "    if epoch%100==0:\n",
    "        print(f\"Average loss in epoch {epoch}: {np.mean(epoch_losses):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R=list()\n",
    "for i in tqdm(range(1000)):\n",
    "    replay,_,_,_=Environment(e=0.1)\n",
    "    r=replay[-1]\n",
    "    R.append(r[2])\n",
    "print(\"Evaluated Average Reward from Greedy policy of the Q net \",np.mean(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"C:/Users/YYC/Desktop/DQN_RLcoin/models/rnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
